{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaec8e14",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This script demonstrates a simple worked example of **backpropagation** in a neural network with:\n",
    "\n",
    "- 1 input neuron\n",
    "- 1 hidden layer with 2 neurons (sigmoid activation)\n",
    "- 1 output neuron (sigmoid)\n",
    "- Mean Squared Error (MSE) loss\n",
    "\n",
    "We manually compute gradients using the chain rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c1dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit  # Numerically stable sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda11de2",
   "metadata": {},
   "source": [
    "# Step 1: Initialize Parameters\n",
    "\n",
    "We define:\n",
    "\n",
    "- Input value: $x = 1$\n",
    "- True label: $y_{\\text{true}} = 0$\n",
    "- Weights:\n",
    "  - $w_1$, $w_2$: input to hidden layer\n",
    "  - $w_3$, $w_4$: hidden to output layer\n",
    "\n",
    "All weights are initialized manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and true label\n",
    "x = np.array([[1.0]])          # Shape (1, 1)\n",
    "y_true = np.array([[0.0]])     # Shape (1, 1)\n",
    "\n",
    "# Initialize weights\n",
    "w1 = np.array([[0.5, -0.5]])   # @ is the dot product: Input to hidden: shape (1, 2)\n",
    "w2 = np.array([[0.3], [-0.3]]) # Hidden to output: shape (2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131299d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return expit(x)\n",
    "# expit(x) = 1 / (1 + exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96fac49",
   "metadata": {},
   "source": [
    "# Step 2: Forward Pass\n",
    "\n",
    "We compute the outputs of the network layer by layer:\n",
    "\n",
    "1. Compute hidden layer pre-activations:\n",
    "   $z_1 = w_1 \\cdot x$, $z_2 = w_2 \\cdot x$\n",
    "2. Apply sigmoid activation:\n",
    "   $h_1 = \\sigma(z_1)$, $h_2 = \\sigma(z_2)$\n",
    "3. Compute output pre-activation:\n",
    "   $z_3 = w_3 \\cdot h_1 + w_4 \\cdot h_2$\n",
    "4. Apply sigmoid again:\n",
    "   $y = \\sigma(z_3)$\n",
    "5. Compute MSE:\n",
    "   $L = \\frac{1}{2}(y - y_{\\text{true}})^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "z1 = x @ w1              # Pre-activation: shape (1, 2)\n",
    "a1 = sigmoid(z1)         # Hidden layer activations: shape (1, 2)\n",
    "z2 = a1 @ w2             # Output layer pre-activation: shape (1, 1)\n",
    "y_pred = sigmoid(z2)     # Output prediction\n",
    "\n",
    "# Loss\n",
    "loss = 0.5 * (y_pred - y_true) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0aaff",
   "metadata": {},
   "source": [
    "# Step 3: Backward Pass\n",
    "\n",
    "We apply the chain rule to compute gradients of the loss w.r.t. each weight:\n",
    "\n",
    "- Output layer:\n",
    "  $\\frac{\\partial L}{\\partial w_3} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_3}$\n",
    "- Hidden layer:\n",
    "  $\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1}$\n",
    "\n",
    "We repeat the same for all weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1043c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "dL_dy = y_pred - y_true              # ∂L/∂y\n",
    "dy_dz2 = sigmoid_derivative(z2)      # ∂y/∂z2\n",
    "\n",
    "# Gradients for w2\n",
    "dz2_dw2 = a1.T                       # ∂z2/∂w2\n",
    "dL_dw2 = dz2_dw2 @ (dL_dy * dy_dz2)  # Shape (2, 1)\n",
    "\n",
    "# Gradients for w1\n",
    "dz2_da1 = w2.T                       # ∂z2/∂a1\n",
    "da1_dz1 = sigmoid_derivative(z1)     # ∂a1/∂z1\n",
    "dz1_dw1 = x.T                        # ∂z1/∂w1\n",
    "dL_dw1 = dz1_dw1.T @ ((dL_dy @ dz2_da1) * da1_dz1)  # Shape (1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c08748",
   "metadata": {},
   "source": [
    "# Step 4: Output\n",
    "\n",
    "We print:\n",
    "\n",
    "- Loss value\n",
    "- Gradient of the loss with respect to:\n",
    "  - $w_1$, $w_2$ (input to hidden layer)\n",
    "  - $w_3$, $w_4$ (hidden to output layer)\n",
    "\n",
    "These gradients can be used in **gradient descent** to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae560cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.13434887664395717\n",
      "Gradient for w1 (input to hidden): [ 0.036545 -0.036545]\n",
      "Gradient for w2 (hidden to output): [0.08055583 0.04885958]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss:\", loss.item())\n",
    "print(\"Gradient for w1 (input to hidden):\", dL_dw1.flatten())\n",
    "print(\"Gradient for w2 (hidden to output):\", dL_dw2.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773e2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
