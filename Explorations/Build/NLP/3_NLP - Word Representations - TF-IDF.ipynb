{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows a method of word representation for NLP related problems and data analysis called **TF-IDF** which is a short of term frequency–inverse document frequency.\n",
    "\n",
    "It is an improved concept of **Bag of Words** which treats each word equaly. **TF-IDF** is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "**TF-IDF** equation:\n",
    "\n",
    "- term frequency\n",
    "\n",
    "$ tf_{i,j} = \\frac{n_{i,j}}{\\sum_k n_{k,j}} $\n",
    "\n",
    "- inverse document frequency\n",
    "\n",
    "$ idf(w) = \\mbox{log} \\frac{N}{df_i} $\n",
    "\n",
    "- term frequency–inverse document frequency\n",
    "\n",
    "$ w_{i,j} = tf_{i,j} \\times \\mbox{log}\\frac{N}{df_i} $\n",
    "\n",
    "where:\n",
    "- $i$ - index of term\n",
    "- $j$ - index of document\n",
    "- $k$ - number of terms in document\n",
    "- $N$ - corpus length (number of documents)\n",
    "- $df_i$ - number of documents containing term i\n",
    "\n",
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:56:57.903798Z",
     "start_time": "2020-03-01T01:56:57.900602Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The dog barks in the morning.\",\n",
    "    \"Over the sofa lies sleeping dog.\",\n",
    "    \"My dog name is Farell, it is very energetic.\",\n",
    "    \"The dog barks at the cars.\",\n",
    "    \"Cat dislikes vegetables.\",\n",
    "    \"Cats sleep during day and hunt during night.\",\n",
    "    \"Cats, dogs and elephants are animals.\",\n",
    "    \"Dogs can run quickly.\",\n",
    "    \"My favourite animals are dogs.\",\n",
    "    \"There are many different animals in the world.\",\n",
    "    \"When I buy a house I will also adopt two cats.\",\n",
    "    \"On cat is black and the other cat is white.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:56:58.697535Z",
     "start_time": "2020-03-01T01:56:57.905818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RhysL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RhysL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RhysL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "for package in [\"punkt\", \"wordnet\", \"stopwords\"]:\n",
    "    nltk.download(package)\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wodnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_document(document, stemmer=porter_stemmer, lemmatizer=wodnet_lemmatizer):\n",
    "    \"\"\"Noramlizes data by performing following steps:\n",
    "        1. Changing each word in corpus to lowercase.\n",
    "        2. Removing special characters and interpunction.\n",
    "        3. Dividing text into tokens.\n",
    "        4. Removing english stopwords.\n",
    "        5. Stemming words.\n",
    "        6. Lemmatizing words.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = document.lower()\n",
    "    temp = re.sub(r\"[^a-zA-Z0-9]\", \" \", temp)\n",
    "    temp = word_tokenize(temp)\n",
    "    temp = [t for t in temp if t not in stopwords.words(\"english\")]\n",
    "    temp = [porter_stemmer.stem(token) for token in temp]\n",
    "    temp = [lemmatizer.lemmatize(token) for token in temp]\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previeving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.413144Z",
     "start_time": "2020-03-01T01:56:58.699762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 The dog barks in the morning.  ->  ['dog', 'bark', 'morn']\n",
      "              Over the sofa lies sleeping dog.  ->  ['sofa', 'lie', 'sleep', 'dog']\n",
      "  My dog name is Farell, it is very energetic.  ->  ['dog', 'name', 'farel', 'energet']\n",
      "                    The dog barks at the cars.  ->  ['dog', 'bark', 'car']\n",
      "                      Cat dislikes vegetables.  ->  ['cat', 'dislik', 'veget']\n",
      "  Cats sleep during day and hunt during night.  ->  ['cat', 'sleep', 'day', 'hunt', 'night']\n",
      "         Cats, dogs and elephants are animals.  ->  ['cat', 'dog', 'eleph', 'anim']\n",
      "                         Dogs can run quickly.  ->  ['dog', 'run', 'quickli']\n",
      "                My favourite animals are dogs.  ->  ['favourit', 'anim', 'dog']\n",
      "There are many different animals in the world.  ->  ['mani', 'differ', 'anim', 'world']\n",
      "When I buy a house I will also adopt two cats.  ->  ['buy', 'hous', 'also', 'adopt', 'two', 'cat']\n",
      "   On cat is black and the other cat is white.  ->  ['cat', 'black', 'cat', 'white']\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document in corpus:\n",
    "    print(document.rjust(offset), \" -> \", normalize_document(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to observe what tokens are left from each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating the CountVectorizer model and removing English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.417093Z",
     "start_time": "2020-03-01T01:57:00.414795Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer(tokenizer=normalize_document)\n",
    "bow.fit(corpus)\n",
    "corpus_vectorized = bow.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Bag Of Words based on corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.443510Z",
     "start_time": "2020-03-01T01:57:00.418519Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RhysL\\miniconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(tokenizer=&lt;function normalize_document at 0x000002053B9A3010&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(tokenizer=&lt;function normalize_document at 0x000002053B9A3010&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(tokenizer=<function normalize_document at 0x000002053B9A3010>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing tokens in the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.448713Z",
     "start_time": "2020-03-01T01:57:00.445453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adopt', 'also', 'anim', 'bark', 'black', 'buy', 'car', 'cat', 'day', 'differ', 'dislik', 'dog', 'eleph', 'energet', 'farel', 'favourit', 'hous', 'hunt', 'lie', 'mani', 'morn', 'name', 'night', 'quickli', 'run', 'sleep', 'sofa', 'two', 'veget', 'white', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(bow.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to see te size of the bag is 31 as there are 31 tokens inside of it. Because of that each sentence will be represented with vector of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.469821Z",
     "start_time": "2020-03-01T01:57:00.450422Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_vectorized = bow.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.480964Z",
     "start_time": "2020-03-01T01:57:00.472388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 The dog barks in the morning.  ->  [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "              Over the sofa lies sleeping dog.  ->  [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      "  My dog name is Farell, it is very energetic.  ->  [0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "                    The dog barks at the cars.  ->  [0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                      Cat dislikes vegetables.  ->  [0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "  Cats sleep during day and hunt during night.  ->  [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0]\n",
      "         Cats, dogs and elephants are animals.  ->  [0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                         Dogs can run quickly.  ->  [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "                My favourite animals are dogs.  ->  [0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "There are many different animals in the world.  ->  [0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      "When I buy a house I will also adopt two cats.  ->  [1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "   On cat is black and the other cat is white.  ->  [0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document, document_vector in zip(corpus, corpus_vectorized.toarray()):\n",
    "    print(document.rjust(offset), \" -> \", document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such vectors are now representing sentences in corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF-IDF values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:09:42.687503Z",
     "start_time": "2020-03-01T01:09:42.683642Z"
    }
   },
   "source": [
    "Initializing Tfidf transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.485313Z",
     "start_time": "2020-03-01T01:57:00.482884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_idf_transformer = TfidfTransformer()\n",
    "tf_idf_transformer.fit(corpus_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.492420Z",
     "start_time": "2020-03-01T01:57:00.487475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfTransformer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_transformer.fit(corpus_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising frequencies per term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.499060Z",
     "start_time": "2020-03-01T01:57:00.493961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.87180218, 2.87180218, 2.178655  , 2.46633707, 2.87180218,\n",
       "       2.87180218, 2.87180218, 1.77318989, 2.87180218, 2.87180218,\n",
       "       2.87180218, 1.48550782, 2.87180218, 2.87180218, 2.87180218,\n",
       "       2.87180218, 2.87180218, 2.87180218, 2.87180218, 2.87180218,\n",
       "       2.87180218, 2.87180218, 2.87180218, 2.87180218, 2.87180218,\n",
       "       2.46633707, 2.87180218, 2.87180218, 2.87180218, 2.87180218,\n",
       "       2.87180218])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.512503Z",
     "start_time": "2020-03-01T01:57:00.500637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     adopt  :  2.8718021769015913\n",
      "      also  :  2.8718021769015913\n",
      "      anim  :  2.1786549963416464\n",
      "      bark  :  2.466337068793427\n",
      "     black  :  2.8718021769015913\n",
      "       buy  :  2.8718021769015913\n",
      "       car  :  2.8718021769015913\n",
      "       cat  :  1.7731898882334818\n",
      "       day  :  2.8718021769015913\n",
      "    differ  :  2.8718021769015913\n",
      "    dislik  :  2.8718021769015913\n",
      "       dog  :  1.4855078157817008\n",
      "     eleph  :  2.8718021769015913\n",
      "   energet  :  2.8718021769015913\n",
      "     farel  :  2.8718021769015913\n",
      "  favourit  :  2.8718021769015913\n",
      "      hous  :  2.8718021769015913\n",
      "      hunt  :  2.8718021769015913\n",
      "       lie  :  2.8718021769015913\n",
      "      mani  :  2.8718021769015913\n",
      "      morn  :  2.8718021769015913\n",
      "      name  :  2.8718021769015913\n",
      "     night  :  2.8718021769015913\n",
      "   quickli  :  2.8718021769015913\n",
      "       run  :  2.8718021769015913\n",
      "     sleep  :  2.466337068793427\n",
      "      sofa  :  2.8718021769015913\n",
      "       two  :  2.8718021769015913\n",
      "     veget  :  2.8718021769015913\n",
      "     white  :  2.8718021769015913\n",
      "     world  :  2.8718021769015913\n"
     ]
    }
   ],
   "source": [
    "for term, freq in zip(bow.get_feature_names_out(), tf_idf_transformer.idf_):\n",
    "    print(term.rjust(10), \" : \", freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising frequency for document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.519471Z",
     "start_time": "2020-03-01T01:57:00.516367Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_docs = tf_idf_transformer.transform(corpus_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.528954Z",
     "start_time": "2020-03-01T01:57:00.521405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 31)\n",
      "[[0.         0.         0.         0.60649426 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.36529961\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.70620175 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.29839313\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.57685731 0.         0.         0.         0.         0.\n",
      "  0.         0.49541176 0.57685731 0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.28615928\n",
      "  0.         0.55320667 0.55320667 0.         0.         0.\n",
      "  0.         0.         0.         0.55320667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.60649426 0.         0.\n",
      "  0.70620175 0.         0.         0.         0.         0.36529961\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.40012796 0.         0.         0.64803457 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.64803457 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30423934 0.4927364  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4927364\n",
      "  0.         0.         0.         0.         0.4927364  0.\n",
      "  0.         0.42316774 0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.50866623 0.         0.         0.\n",
      "  0.         0.41399938 0.         0.         0.         0.34683218\n",
      "  0.67050028 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.34351038\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.66407854\n",
      "  0.66407854 0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.55880368 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.38101821\n",
      "  0.         0.         0.         0.73658915 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.4012026  0.         0.         0.\n",
      "  0.         0.         0.         0.52884669 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.52884669 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.52884669]\n",
      " [0.43108083 0.43108083 0.         0.         0.         0.43108083\n",
      "  0.         0.2661702  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.43108083 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.43108083 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.53262587 0.\n",
      "  0.         0.65773807 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.53262587\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_docs.toarray().shape)\n",
    "print(tfidf_docs.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:57:00.706269Z",
     "start_time": "2020-03-01T01:57:00.530581Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id.0: The dog barks in the morning.\n",
      "Tokens: ['dog', 'bark', 'morn']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.60649426]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.36529961]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.70620175]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.1: Over the sofa lies sleeping dog.\n",
      "Tokens: ['sofa', 'lie', 'sleep', 'dog']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.29839313]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.57685731]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.49541176]\n",
      "      sofa  :  [0.57685731]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.2: My dog name is Farell, it is very energetic.\n",
      "Tokens: ['dog', 'name', 'farel', 'energet']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.28615928]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.55320667]\n",
      "     farel  :  [0.55320667]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.55320667]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.3: The dog barks at the cars.\n",
      "Tokens: ['dog', 'bark', 'car']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.60649426]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.70620175]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.36529961]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.4: Cat dislikes vegetables.\n",
      "Tokens: ['cat', 'dislik', 'veget']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.40012796]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.64803457]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.64803457]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.5: Cats sleep during day and hunt during night.\n",
      "Tokens: ['cat', 'sleep', 'day', 'hunt', 'night']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.30423934]\n",
      "       day  :  [0.4927364]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.4927364]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.4927364]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.42316774]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.6: Cats, dogs and elephants are animals.\n",
      "Tokens: ['cat', 'dog', 'eleph', 'anim']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.50866623]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.41399938]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.34683218]\n",
      "     eleph  :  [0.67050028]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.7: Dogs can run quickly.\n",
      "Tokens: ['dog', 'run', 'quickli']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.34351038]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.66407854]\n",
      "       run  :  [0.66407854]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.8: My favourite animals are dogs.\n",
      "Tokens: ['favourit', 'anim', 'dog']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.55880368]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.38101821]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.73658915]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.9: There are many different animals in the world.\n",
      "Tokens: ['mani', 'differ', 'anim', 'world']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.4012026]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.52884669]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.52884669]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.52884669]\n",
      "\n",
      " ------------------\n",
      "Document id.10: When I buy a house I will also adopt two cats.\n",
      "Tokens: ['buy', 'hous', 'also', 'adopt', 'two', 'cat']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.43108083]\n",
      "      also  :  [0.43108083]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.43108083]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.2661702]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.43108083]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.43108083]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.11: On cat is black and the other cat is white.\n",
      "Tokens: ['cat', 'black', 'cat', 'white']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.53262587]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.65773807]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.53262587]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n"
     ]
    }
   ],
   "source": [
    "tfidf_docs = tf_idf_transformer.transform(corpus_vectorized)\n",
    "\n",
    "for doc_id in range(len(corpus)):\n",
    "    print(\"Document id.{}: {}\".format(doc_id, corpus[doc_id]))\n",
    "    print(\"Tokens: {}\".format(normalize_document(corpus[doc_id])))\n",
    "    print(\"\\n -- TF IDF Values for words in dictionary:\")\n",
    "    for term, freq in zip(bow.get_feature_names_out(), tfidf_docs[doc_id].T.toarray()):\n",
    "        print(term.rjust(10), \" : \", freq)\n",
    "    print(\"\\n ------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id.0: The dog barks in the morning.\n",
      "Tokens: ['dog', 'bark', 'morn']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "      bark  :  [0.60649426]\n",
      "       dog  :  [0.36529961]\n",
      "      morn  :  [0.70620175]\n",
      "\n",
      " ------------------\n",
      "Document id.1: Over the sofa lies sleeping dog.\n",
      "Tokens: ['sofa', 'lie', 'sleep', 'dog']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "       dog  :  [0.29839313]\n",
      "       lie  :  [0.57685731]\n",
      "     sleep  :  [0.49541176]\n",
      "      sofa  :  [0.57685731]\n",
      "\n",
      " ------------------\n",
      "Document id.2: My dog name is Farell, it is very energetic.\n",
      "Tokens: ['dog', 'name', 'farel', 'energet']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "       dog  :  [0.28615928]\n",
      "   energet  :  [0.55320667]\n",
      "     farel  :  [0.55320667]\n",
      "      name  :  [0.55320667]\n",
      "\n",
      " ------------------\n",
      "Document id.3: The dog barks at the cars.\n",
      "Tokens: ['dog', 'bark', 'car']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "      bark  :  [0.60649426]\n",
      "       car  :  [0.70620175]\n",
      "       dog  :  [0.36529961]\n",
      "\n",
      " ------------------\n",
      "Document id.4: Cat dislikes vegetables.\n",
      "Tokens: ['cat', 'dislik', 'veget']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "       cat  :  [0.40012796]\n",
      "    dislik  :  [0.64803457]\n",
      "     veget  :  [0.64803457]\n",
      "\n",
      " ------------------\n",
      "Document id.5: Cats sleep during day and hunt during night.\n",
      "Tokens: ['cat', 'sleep', 'day', 'hunt', 'night']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "       cat  :  [0.30423934]\n",
      "       day  :  [0.4927364]\n",
      "      hunt  :  [0.4927364]\n",
      "     night  :  [0.4927364]\n",
      "     sleep  :  [0.42316774]\n",
      "\n",
      " ------------------\n",
      "Document id.6: Cats, dogs and elephants are animals.\n",
      "Tokens: ['cat', 'dog', 'eleph', 'anim']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "      anim  :  [0.50866623]\n",
      "       cat  :  [0.41399938]\n",
      "       dog  :  [0.34683218]\n",
      "     eleph  :  [0.67050028]\n",
      "\n",
      " ------------------\n",
      "Document id.7: Dogs can run quickly.\n",
      "Tokens: ['dog', 'run', 'quickli']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "       dog  :  [0.34351038]\n",
      "   quickli  :  [0.66407854]\n",
      "       run  :  [0.66407854]\n",
      "\n",
      " ------------------\n",
      "Document id.8: My favourite animals are dogs.\n",
      "Tokens: ['favourit', 'anim', 'dog']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "      anim  :  [0.55880368]\n",
      "       dog  :  [0.38101821]\n",
      "  favourit  :  [0.73658915]\n",
      "\n",
      " ------------------\n",
      "Document id.9: There are many different animals in the world.\n",
      "Tokens: ['mani', 'differ', 'anim', 'world']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "      anim  :  [0.4012026]\n",
      "    differ  :  [0.52884669]\n",
      "      mani  :  [0.52884669]\n",
      "     world  :  [0.52884669]\n",
      "\n",
      " ------------------\n",
      "Document id.10: When I buy a house I will also adopt two cats.\n",
      "Tokens: ['buy', 'hous', 'also', 'adopt', 'two', 'cat']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.43108083]\n",
      "      also  :  [0.43108083]\n",
      "       buy  :  [0.43108083]\n",
      "       cat  :  [0.2661702]\n",
      "      hous  :  [0.43108083]\n",
      "       two  :  [0.43108083]\n",
      "\n",
      " ------------------\n",
      "Document id.11: On cat is black and the other cat is white.\n",
      "Tokens: ['cat', 'black', 'cat', 'white']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     black  :  [0.53262587]\n",
      "       cat  :  [0.65773807]\n",
      "     white  :  [0.53262587]\n",
      "\n",
      " ------------------\n"
     ]
    }
   ],
   "source": [
    "tfidf_docs = tf_idf_transformer.transform(corpus_vectorized)\n",
    "\n",
    "for doc_id in range(len(corpus)):\n",
    "    print(\"Document id.{}: {}\".format(doc_id, corpus[doc_id]))\n",
    "    print(\"Tokens: {}\".format(normalize_document(corpus[doc_id])))\n",
    "    print(\"\\n -- TF IDF Values for words in dictionary:\")\n",
    "    \n",
    "    # Filter out terms with TF-IDF frequency of 0\n",
    "    non_zero_terms = [(term, freq) for term, freq in zip(bow.get_feature_names_out(), tfidf_docs[doc_id].T.toarray()) if freq != 0]\n",
    "\n",
    "    for term, freq in non_zero_terms:\n",
    "        print(term.rjust(10), \" : \", freq)\n",
    "    \n",
    "    print(\"\\n ------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
