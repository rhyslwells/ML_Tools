{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc0bafb",
   "metadata": {},
   "source": [
    "What you’re seeing is a classic example of **misclassification due to a very small dataset** and the limitations of a simple model like `MultinomialNB` on text features. Let me break it down step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **How the Model Sees the Sentences**\n",
    "\n",
    "The model doesn’t “understand” language. It only sees **numbers representing word importance** (TF-IDF scores).\n",
    "\n",
    "For example:\n",
    "\n",
    "* `'I am learning neural networks.'` → contains words like `learning`, `neural`, `networks`. These words were likely **only seen in Tech sentences** in your small training set. So the model predicts **Tech**, which is correct.\n",
    "\n",
    "* `'It's raining heavily.'` → contains `raining`, `heavily`. These words **weren’t in your training set**, so the model tries to match whatever words are closest.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Why `'It's raining heavily.'` was classified as Tech**\n",
    "\n",
    "* With only **10 sentences**, there are **very few examples of Non-Tech words**.\n",
    "* TF-IDF assigns **0 weight to words not seen in training**, so effectively the vector for this sentence is mostly zeros.\n",
    "* The Naive Bayes classifier defaults to predicting the **most frequent class in the training set**, which is Tech in this tiny sample.\n",
    "\n",
    "This is why it predicted **Tech incorrectly**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Limitations of this approach**\n",
    "\n",
    "* **Tiny dataset**: 10 sentences is far too few to generalize.\n",
    "* **Simple features**: TF-IDF only captures word frequency, not context.\n",
    "* **Simple model**: Naive Bayes assumes **words are independent**—it doesn’t understand phrases or sentence meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **How to improve**\n",
    "\n",
    "* **Add more labeled data**: At least hundreds of sentences per class.\n",
    "* **Use richer representations**: Word embeddings (Word2Vec, GloVe) or transformers (BERT) capture meaning, so “raining” is understood as weather-related.\n",
    "* **Handle unknown words**: More training data reduces zero-feature issues.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can write a **small BERT-based classifier** that would correctly classify `'It's raining heavily.'` as Non-Tech even with a small dataset. It’s much more robust than TF-IDF + Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0260c933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "# NLP Exploration: Sentence Classification\n",
    "#---------------------------------------------\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ----------------- DATA -----------------\n",
    "# Example sentences (replace with your actual sentences)\n",
    "sentences = [\n",
    "    \"I love reading books on data science.\",\n",
    "    \"The weather today is sunny and warm.\",\n",
    "    \"Python is a great programming language for AI.\",\n",
    "    \"I need to buy groceries after work.\",\n",
    "    \"The new movie was fantastic!\",\n",
    "    \"Machine learning models require careful tuning.\",\n",
    "    \"My car broke down on the way home.\",\n",
    "    \"Data visualization helps understand trends.\",\n",
    "    \"I enjoy hiking during the weekends.\",\n",
    "    \"Artificial intelligence is transforming industries.\"\n",
    "]\n",
    "\n",
    "# Example labels for classification (mock labels for demonstration)\n",
    "# Suppose we want to classify sentences as 'Tech' or 'Non-Tech'\n",
    "labels = [\n",
    "    \"Tech\", \"Non-Tech\", \"Tech\", \"Non-Tech\", \"Non-Tech\",\n",
    "    \"Tech\", \"Non-Tech\", \"Tech\", \"Non-Tech\", \"Tech\"\n",
    "]\n",
    "\n",
    "# Convert to a pandas DataFrame for easier handling\n",
    "df = pd.DataFrame({\"sentence\": sentences, \"label\": labels})\n",
    "print(\"Data preview:\\n\", df.head())\n",
    "\n",
    "# ----------------- PREPROCESSING -----------------\n",
    "# Convert text into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # Remove common words like 'the', 'is'\n",
    "X = vectorizer.fit_transform(df['sentence'])        # Features matrix\n",
    "y = df['label']                                     # Target labels\n",
    "\n",
    "print(\"\\nFeature names (sample):\", vectorizer.get_feature_names_out()[:10])\n",
    "print(\"Shape of features matrix:\", X.shape)\n",
    "\n",
    "# ----------------- SPLIT DATA -----------------\n",
    "# Normally, we split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------- MODEL TRAINING -----------------\n",
    "# Use a simple Naive Bayes classifier suitable for text data\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------- PREDICTION -----------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ----------------- EVALUATION -----------------\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# ----------------- PREDICT NEW SENTENCES -----------------\n",
    "new_sentences = [\n",
    "    \"I am learning neural networks.\",\n",
    "    \"It's raining heavily.\"\n",
    "]\n",
    "X_new = vectorizer.transform(new_sentences)\n",
    "predictions = model.predict(X_new)\n",
    "\n",
    "for sent, pred in zip(new_sentences, predictions):\n",
    "    print(f\"Sentence: '{sent}' -> Predicted Label: {pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
