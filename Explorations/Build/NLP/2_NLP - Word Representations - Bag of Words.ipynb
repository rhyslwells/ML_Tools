{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows a method of word representation for NLP related problems and data analysis called **Bag of Words**.\n",
    "\n",
    "It treats each document or text as an unordered collection. For example if the goal is to analyze tweets from Twitter then each separate tweet is a document in this case.\n",
    "\n",
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:40.196282Z",
     "start_time": "2020-03-01T00:19:40.193255Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The dog barks in the morning.\",\n",
    "    \"Over the sofa lies sleeping dog.\",\n",
    "    \"My dog name is Farell, it is very energetic.\",\n",
    "    \"The dog barks at the cars.\",\n",
    "    \"Cat dislikes vegetables.\",\n",
    "    \"Cats sleep during day and hunt during night.\",\n",
    "    \"Cats and dogs are not getting along. I prefer cats.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:41.021975Z",
     "start_time": "2020-03-01T00:19:40.198208Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RhysL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RhysL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RhysL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "for package in [\"punkt\", \"wordnet\", \"stopwords\"]:\n",
    "    nltk.download(package)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wodnet_lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_document(document, stemmer=porter_stemmer, lemmatizer=wodnet_lemmatizer):\n",
    "    \"\"\"Noramlizes data by performing following steps:\n",
    "        1. Changing each word in corpus to lowercase.\n",
    "        2. Removing special characters and interpunction.\n",
    "        3. Dividing text into tokens.\n",
    "        4. Removing english stopwords.\n",
    "        5. Stemming words.\n",
    "        6. Lemmatizing words.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = document.lower()\n",
    "    temp = re.sub(r\"[^a-zA-Z0-9]\", \" \", temp)\n",
    "    temp = word_tokenize(temp)\n",
    "    temp = [t for t in temp if t not in stopwords.words(\"english\")]\n",
    "    temp = [porter_stemmer.stem(token) for token in temp]\n",
    "    temp = [lemmatizer.lemmatize(token) for token in temp]\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previeving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.666191Z",
     "start_time": "2020-03-01T00:19:41.023964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The dog barks in the morning.  ->  ['dog', 'bark', 'morn']\n",
      "                   Over the sofa lies sleeping dog.  ->  ['sofa', 'lie', 'sleep', 'dog']\n",
      "       My dog name is Farell, it is very energetic.  ->  ['dog', 'name', 'farel', 'energet']\n",
      "                         The dog barks at the cars.  ->  ['dog', 'bark', 'car']\n",
      "                           Cat dislikes vegetables.  ->  ['cat', 'dislik', 'veget']\n",
      "       Cats sleep during day and hunt during night.  ->  ['cat', 'sleep', 'day', 'hunt', 'night']\n",
      "Cats and dogs are not getting along. I prefer cats.  ->  ['cat', 'dog', 'get', 'along', 'prefer', 'cat']\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document in corpus:\n",
    "    print(document.rjust(offset), \" -> \", normalize_document(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to observe what tokens are left from each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating the CountVectorizer model and removing English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.670191Z",
     "start_time": "2020-03-01T00:19:42.668028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along' 'bark' 'car' 'cat' 'day' 'dislik' 'dog' 'energet' 'farel' 'get'\n",
      " 'hunt' 'lie' 'morn' 'name' 'night' 'prefer' 'sleep' 'sofa' 'veget']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RhysL\\miniconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using CountVectorizer with the custom tokenizer\n",
    "bow = CountVectorizer(tokenizer=normalize_document)\n",
    "bow.fit(corpus)  # Fitting text to this model\n",
    "print(bow.get_feature_names_out())  # Key terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Bag Of Words based on corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.691124Z",
     "start_time": "2020-03-01T00:19:42.671714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(tokenizer=&lt;function normalize_document at 0x0000026A168E30A0&gt;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(tokenizer=&lt;function normalize_document at 0x0000026A168E30A0&gt;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(tokenizer=<function normalize_document at 0x0000026A168E30A0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.fit(corpus) # fitting text to this model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing tokens in the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.695798Z",
     "start_time": "2020-03-01T00:19:42.692737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along' 'bark' 'car' 'cat' 'day' 'dislik' 'dog' 'energet' 'farel' 'get'\n",
      " 'hunt' 'lie' 'morn' 'name' 'night' 'prefer' 'sleep' 'sofa' 'veget']\n"
     ]
    }
   ],
   "source": [
    "print(bow.get_feature_names_out()) # key terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to see te size of the bag is 16 as there are 16 tokens inside of it. Because of that each sentence will be represented with vector of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.712626Z",
     "start_time": "2020-03-01T00:19:42.697281Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_vectorized = bow.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.726257Z",
     "start_time": "2020-03-01T00:19:42.716029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The dog barks in the morning.  ->  [0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "                   Over the sofa lies sleeping dog.  ->  [0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0]\n",
      "       My dog name is Farell, it is very energetic.  ->  [0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0]\n",
      "                         The dog barks at the cars.  ->  [0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                           Cat dislikes vegetables.  ->  [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "       Cats sleep during day and hunt during night.  ->  [0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0]\n",
      "Cats and dogs are not getting along. I prefer cats.  ->  [1 0 0 2 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document, document_vector in zip(corpus, corpus_vectorized.toarray()):\n",
    "    print(document.rjust(offset), \" -> \", document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such vectors are now representing sentences in corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is desired to don't include counts of words but rather whether word from dictionary/bag is in sentence or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.751580Z",
     "start_time": "2020-03-01T00:19:42.730131Z"
    }
   },
   "outputs": [],
   "source": [
    "bow_ohe = CountVectorizer(tokenizer=normalize_document, binary=True)\n",
    "corpus_vectorized_ohe = bow_ohe.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.761827Z",
     "start_time": "2020-03-01T00:19:42.753647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The dog barks in the morning.  ->  [0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "                   Over the sofa lies sleeping dog.  ->  [0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0]\n",
      "       My dog name is Farell, it is very energetic.  ->  [0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0]\n",
      "                         The dog barks at the cars.  ->  [0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                           Cat dislikes vegetables.  ->  [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "       Cats sleep during day and hunt during night.  ->  [0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0]\n",
      "Cats and dogs are not getting along. I prefer cats.  ->  [1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document, document_vector in zip(corpus, corpus_vectorized_ohe.toarray()):\n",
    "    print(document.rjust(offset), \" -> \", document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams allows to create tokens from group of words to get more information about their context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:11:47.458593Z",
     "start_time": "2020-03-01T00:11:47.454929Z"
    }
   },
   "source": [
    "- ngram range (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.782614Z",
     "start_time": "2020-03-01T00:19:42.764560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along', 'along prefer', 'bark', 'bark car', 'bark morn', 'car', 'cat', 'cat dislik', 'cat dog', 'cat sleep', 'day', 'day hunt', 'dislik', 'dislik veget', 'dog', 'dog bark', 'dog get', 'dog name', 'energet', 'farel', 'farel energet', 'get', 'get along', 'hunt', 'hunt night', 'lie', 'lie sleep', 'morn', 'name', 'name farel', 'night', 'prefer', 'prefer cat', 'sleep', 'sleep day', 'sleep dog', 'sofa', 'sofa lie', 'veget']\n"
     ]
    }
   ],
   "source": [
    "bow_ngram_12 = CountVectorizer(tokenizer=normalize_document, ngram_range=(1,2))\n",
    "bow_ngram_12.fit(corpus)\n",
    "print(bow_ngram_12.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram range (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.804588Z",
     "start_time": "2020-03-01T00:19:42.784571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along prefer', 'bark car', 'bark morn', 'cat dislik', 'cat dog', 'cat sleep', 'day hunt', 'dislik veget', 'dog bark', 'dog get', 'dog name', 'farel energet', 'get along', 'hunt night', 'lie sleep', 'name farel', 'prefer cat', 'sleep day', 'sleep dog', 'sofa lie']\n"
     ]
    }
   ],
   "source": [
    "bow_ngram_22 = CountVectorizer(tokenizer=normalize_document, ngram_range=(2,2))\n",
    "bow_ngram_22.fit(corpus)\n",
    "print(bow_ngram_22.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram range (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.827462Z",
     "start_time": "2020-03-01T00:19:42.806829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along', 'along prefer', 'along prefer cat', 'bark', 'bark car', 'bark morn', 'car', 'cat', 'cat dislik', 'cat dislik veget', 'cat dog', 'cat dog get', 'cat sleep', 'cat sleep day', 'day', 'day hunt', 'day hunt night', 'dislik', 'dislik veget', 'dog', 'dog bark', 'dog bark car', 'dog bark morn', 'dog get', 'dog get along', 'dog name', 'dog name farel', 'energet', 'farel', 'farel energet', 'get', 'get along', 'get along prefer', 'hunt', 'hunt night', 'lie', 'lie sleep', 'lie sleep dog', 'morn', 'name', 'name farel', 'name farel energet', 'night', 'prefer', 'prefer cat', 'sleep', 'sleep day', 'sleep day hunt', 'sleep dog', 'sofa', 'sofa lie', 'sofa lie sleep', 'veget']\n"
     ]
    }
   ],
   "source": [
    "bow_ngram_13 = CountVectorizer(tokenizer=normalize_document, ngram_range=(1,3))\n",
    "bow_ngram_13.fit(corpus)\n",
    "print(bow_ngram_13.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram range (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.851191Z",
     "start_time": "2020-03-01T00:19:42.831154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along prefer', 'along prefer cat', 'bark car', 'bark morn', 'cat dislik', 'cat dislik veget', 'cat dog', 'cat dog get', 'cat sleep', 'cat sleep day', 'day hunt', 'day hunt night', 'dislik veget', 'dog bark', 'dog bark car', 'dog bark morn', 'dog get', 'dog get along', 'dog name', 'dog name farel', 'farel energet', 'get along', 'get along prefer', 'hunt night', 'lie sleep', 'lie sleep dog', 'name farel', 'name farel energet', 'prefer cat', 'sleep day', 'sleep day hunt', 'sleep dog', 'sofa lie', 'sofa lie sleep']\n"
     ]
    }
   ],
   "source": [
    "bow_ngram_23 = CountVectorizer(tokenizer=normalize_document, ngram_range=(2,3))\n",
    "bow_ngram_23.fit(corpus)\n",
    "print(bow_ngram_23.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
