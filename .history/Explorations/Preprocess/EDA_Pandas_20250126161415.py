# -*- coding: utf-8 -*-
"""techniques_eda_pandas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/andresvourakis/tbds-demos/blob/main/5-underrated-pandas-techniques/techniques_eda_pandas.ipynb

5 Underrated Pandas Techniques to Make EDA Easier
===

This notebook demonstrates the code examples from this [article](https://tobeadatascientist.substack.com/p/5-underrated-pandas-techniques-to-make-eda-easier), showcasing the before and after of each technique.

For more resources like this, visit [tobeadatascientist.com](https://tobeadatascientist.com)

# Import data
"""

import pandas as pd
import numpy as np
import ast

df = pd.read_csv('https://raw.githubusercontent.com/andresvourakis/tbds-demos/refs/heads/main/5-underrated-pandas-techniques/data.csv')

df.head()

"""**This is an e-commerce sales dataset**

Here is what's included:
- order_id: Unique identifier for each order.
- customer_id: Unique identifier for customers.
- order_date: Date of the order.
- region: Region where the order was placed (e.g., North, South, East, West).
- category: Product category (e.g., Electronics, Clothing, Home Decor).
- quantity: Number of units sold.
- price_per_unit: Price per unit of the product.
- tags: Tags assigned to the product (e.g., [“gift”, “discounted”]).
- gender: Gender of the customer (Male, Female, or Non-binary).
- total_sales: Computed as quantity * price_per_unit.

# 1. Detect Outliers with `.quantile()`
"""

# Calculate IQR
Q1 = df['total_sales'].quantile(0.25)
Q3 = df['total_sales'].quantile(0.75)
IQR = Q3 - Q1

# Identify outliers
outliers = df[(df['total_sales'] < Q1 - 1 * IQR) | (df['total_sales'] > Q3 + 1.5 * IQR)]
df_no_outliers = df[~df.index.isin(outliers.index)]  # Exclude rows matching outliers

print("\nOutliers:")
outliers

"""*Find more information in the official [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html)*

# 2. Unpack Nested Data with `.explode()`
"""

# Ensure it is list type and not string
df['tags'] = df['tags'].apply(ast.literal_eval)

# Explode the nested data
exploded_df = df.explode('tags')

print("\nExploded Data:")
exploded_df.head()

"""*Find more information in the official [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.explode.html)*

# 3. Simplify Aggregations with `.agg()`
"""

# Aggregations
summary = df.groupby('region')['total_sales'].agg(['mean', 'sum', 'max'])

print("Aggregated Data:")
print(summary)

"""*Find more information in the official [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.agg.html)*

# 4. Streamline Transformations with `.pipe()`
"""

# Define custom transformation functions
def fill_missing(df):
    # Explicitly assign the modified column back to the DataFrame
    df['quantity'] = df['quantity'].fillna(df['quantity'].median())
    return df

def normalize(df):
    # Explicitly assign the new column to the DataFrame
    df['sales_normalized'] = (df['total_sales'] - df['total_sales'].mean()) / df['total_sales'].std()
    return df

def add_features(df):
    # Explicitly assign the new column to the DataFrame
    df['sales_category'] = pd.cut(df['total_sales'], bins=[0, 500, 1000, float('inf')],
                                  labels=['Low', 'Medium', 'High'])
    return df

# Apply transformations using pipe
df = (df.pipe(fill_missing)
          .pipe(normalize)
          .pipe(add_features))

print("Transformed Data:")
df.head()

"""*Find more information in the official [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html)*

# 5. Analyze Relationships with `.crosstab()`
"""

# Crosstab
crosstab_result = pd.crosstab(df['gender'], df['category'], margins=True)

print("Crosstab Result:")
print(crosstab_result)

"""*Find more information in the official [documentation](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html)*"""


```python
sns.pairplot(
    penguins,
    x_vars=["bill_length_mm"],
    y_vars=["bill_depth_mm"],hue="species")
```

What are the data types for various features?
`df.info()`

Get a summary of missing data of a df.
```python
def get_numerical_summary(df):
    total = df.shape[0]
    missing_columns = [col for col in df.columns if df[col].isnull().sum() > 0]
    missing_percent = {}
    for col in missing_columns:
        null_count = df[col].isnull().sum()
        per = (null_count/total)  100
        missing_percent[col] = per
        print("{} : {} ({}%)".format(col, null_count, round(per, 3)))
    return missing_percent
```


What does 
```python
reg_summary = pd.DataFrame(data = x.columns.values, columns=['Features'])
```
do?

x.columns.values: Assuming `x` is a DataFrame, `x.columns` retrieves the column names of the DataFrame, and `values` converts them into a NumPy array. 

columns=['Features']: This sets the column name for the feature names array in the new DataFrame. The resulting DataFrame has one column named 'Features' containing the names of the features.